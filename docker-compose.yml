services:
  opensearch:
    image: opensearchproject/opensearch:latest
    container_name: opensearch
    environment:
      - discovery.type=single-node
      - plugins.security.disabled=true
      - OPENSEARCH_INITIAL_ADMIN_PASSWORD=${OPENSEARCH_INITIAL_ADMIN_PASSWORD:-StrongPassword123!}
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
      - "9600:9600"
    volumes:
      - opensearch-data:/usr/share/opensearch/data
    networks:
      - rag-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:latest
    container_name: opensearch-dashboards
    ports:
      - "5601:5601"
    environment:
      - 'OPENSEARCH_HOSTS=["http://opensearch:9200"]'
      - DISABLE_SECURITY_DASHBOARDS_PLUGIN=true
    depends_on:
      opensearch:
        condition: service_healthy
    networks:
      - rag-network

  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    environment:
      # Reduce noisy per-request access logs from actix_web middleware logger.
      - RUST_LOG=warn,actix_web=warn
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant-data:/qdrant/storage
    networks:
      - rag-network
    healthcheck:
      test:
        [
          "CMD",
          "bash",
          "-lc",
          "exec 3<>/dev/tcp/localhost/6333; printf 'GET /healthz HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n' >&3; head -n 1 <&3 | grep -q '200'"
        ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  postgres:
    image: postgres:16-alpine
    container_name: postgres
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
      - POSTGRES_DB=document_storage
    ports:
      - "5433:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - rag-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # RustFS (S3-compatible object storage for document-storage)
  rustfs:
    image: rustfs/rustfs:latest
    container_name: rustfs-server
    security_opt:
      - "no-new-privileges:true"
    ports:
      - "9000:9000" # S3 API port
      - "9001:9001" # Console port
    environment:
      - RUSTFS_VOLUMES=/data/rustfs{0...3}
      - RUSTFS_ADDRESS=0.0.0.0:9000
      - RUSTFS_CONSOLE_ADDRESS=0.0.0.0:9001
      - RUSTFS_CONSOLE_ENABLE=true
      - RUSTFS_EXTERNAL_ADDRESS=:9000
      - RUSTFS_CORS_ALLOWED_ORIGINS=*
      - RUSTFS_CONSOLE_CORS_ALLOWED_ORIGINS=*
      - RUSTFS_ACCESS_KEY=${RUSTFS_ACCESS_KEY:-rustfsadmin}
      - RUSTFS_SECRET_KEY=${RUSTFS_SECRET_KEY:-rustfsadmin}
      - RUSTFS_OBS_LOGGER_LEVEL=info
      - RUSTFS_TLS_PATH=/opt/tls
      # Object Cache
      - RUSTFS_OBJECT_CACHE_ENABLE=true
      - RUSTFS_OBJECT_CACHE_TTL_SECS=300
    volumes:
      - rustfs_data_0:/data/rustfs0
      - rustfs_data_1:/data/rustfs1
      - rustfs_data_2:/data/rustfs2
      - rustfs_data_3:/data/rustfs3
      - rustfs_logs:/app/logs
    networks:
      - rag-network
      - rustfs-network
    restart: unless-stopped
    depends_on:
      volume-permission-helper:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "sh", "-c", "curl -f http://localhost:9000/health && curl -f http://localhost:9001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # RustFS volume permissions fixer service
  volume-permission-helper:
    image: alpine
    volumes:
      - rustfs_data_0:/data0
      - rustfs_data_1:/data1
      - rustfs_data_2:/data2
      - rustfs_data_3:/data3
      - rustfs_logs:/logs
    command: >
      sh -c "
        chown -R 10001:10001 /data0 /data1 /data2 /data3 /logs &&
        echo 'Volume Permissions fixed' &&
        exit 0
      "
    restart: "no"

  rabbitmq:
    image: rabbitmq:3-management
    container_name: rabbitmq
    ports:
      - "5672:5672"
      - "15672:15672"
    environment:
      - RABBITMQ_DEFAULT_USER=${RABBITMQ_DEFAULT_USER:-guest}
      - RABBITMQ_DEFAULT_PASS=${RABBITMQ_DEFAULT_PASS:-guest}
    networks:
      - rag-network
    healthcheck:
      test: ["CMD-SHELL", "rabbitmq-diagnostics -q ping"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 10s

  infinity-embed:
    image: michaelf34/infinity:latest
    container_name: infinity-embed
    command:
      [
        "v2",
        "--model-id",
        "intfloat/multilingual-e5-large-instruct",
        "--device",
        "cuda",
        "--device-id",
        "0",
        "--port",
        "7997"
      ]
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HF_HOME=/root/.cache/huggingface
      - DO_NOT_TRACK=1
    networks:
      - rag-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["4"]
              capabilities: [gpu]

  # Stateless Hybrid Retrieval microservice (scale this one for multiple replicas)
  retrieval:
    build:
      context: ./service
    ports:
      - "8080:8080"
    environment:
      - RAG_OS_URL=http://opensearch:9200
      - RAG_QDRANT_URL=http://qdrant:6333
      # Use a separate collection because the embedding model defines vector dim.
      # (existing collections may already be created with a different vector size).
      - RAG_QDRANT_COLLECTION=rag_chunks_1024
      # Embeddings (Infinity inside docker network)
      - RAG_EMBEDDING_PROVIDER=http
      - RAG_EMBEDDING_URL=http://infinity-embed:7997/embeddings
      - RAG_EMBEDDING_MODEL=intfloat/multilingual-e5-large-instruct
      - RAG_VECTOR_SIZE=1024
      # Retrieval tuning (RU RAG eval)
      - RAG_BM25_TOP_K=150
      - RAG_VECTOR_TOP_K=150
      - RAG_MAX_CHUNKS_PER_DOC=8
      # Rerank (Infinity rerank service started by this compose)
      - RAG_RERANK_MODE=always
      - RAG_RERANK_URL=http://infinity-rerank:7998/rerank
      - RAG_RERANK_MODEL=BAAI/bge-reranker-v2-m3
      - RAG_RERANK_MAX_CANDIDATES=100
      - RAG_LOG_LEVEL=INFO
      - RAG_ENABLE_PAGE_DEDUPLICATION=true
      - RAG_ENABLE_PARENT_PAGE_RETRIEVAL=true
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      opensearch:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      infinity-embed:
        condition: service_started
      infinity-rerank:
        condition: service_started
    networks:
      - rag-network

  # Document Storage Service
  document-storage:
    build:
      context: ./document-storage
    ports:
      - "8081:8081"
    environment:
      - STORAGE_STORAGE_BACKEND=s3
      - STORAGE_STORAGE_PATH=/data/documents
      - STORAGE_S3_ENDPOINT=http://rustfs:9000
      - STORAGE_S3_BUCKET=${STORAGE_S3_BUCKET:-documents}
      - STORAGE_S3_ACCESS_KEY=${RUSTFS_ACCESS_KEY:-rustfsadmin}
      - STORAGE_S3_SECRET_KEY=${RUSTFS_SECRET_KEY:-rustfsadmin}
      - STORAGE_S3_REGION=${STORAGE_S3_REGION:-us-east-1}
      - STORAGE_DB_URL=postgresql://postgres:${POSTGRES_PASSWORD:-postgres}@postgres:5432/document_storage
      - STORAGE_MAX_FILE_SIZE_MB=100
      - STORAGE_LOG_LEVEL=INFO
    volumes:
      - document-storage-data:/data/documents
    depends_on:
      postgres:
        condition: service_healthy
      rustfs:
        condition: service_healthy
    networks:
      - rag-network
      - rustfs-network

  # vLLM (OpenAI-compatible) hosting Granite-Docling VLM for document-to-text extraction.
  # NOTE: requires NVIDIA GPU + drivers on the host.
  vllm-docling:
    image: vllm/vllm-openai:latest
    container_name: vllm-docling
    command: ["--model", "ibm-granite/granite-docling-258M", "--port", "8123"]
    ports:
      - "8123:8123"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HF_HOME=/root/.cache/huggingface
    networks:
      - rag-network
    #If your Docker supports it, uncomment GPU settings below:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["2"]
              capabilities: [gpu]

  # Document Processor Service (pulls files from document-storage, uses Granite-Docling via vLLM, indexes into retrieval)
  doc-processor:
    build:
      context: ./doc-processor
    ports:
      - "8082:8082"
    environment:
      - PROCESSOR_STORAGE_URL=http://document-storage:8081
      - PROCESSOR_RETRIEVAL_URL=http://retrieval:8080
      - PROCESSOR_VLM_BASE_URL=http://vllm-docling:8123/v1
      - PROCESSOR_VLM_MODEL=ibm-granite/granite-docling-258M
      - PROCESSOR_LOG_LEVEL=INFO
      - PROCESSOR_MAX_PAGES=25
    depends_on:
      document-storage:
        condition: service_started
      retrieval:
        condition: service_started
      vllm-docling:
        condition: service_started
    networks:
      - rag-network

  ingestion-worker:
    build:
      context: ./doc-processor
    command: ["python", "-m", "app.worker"]
    environment:
      - WORKER_RABBIT_URL=amqp://${RABBITMQ_DEFAULT_USER:-guest}:${RABBITMQ_DEFAULT_PASS:-guest}@rabbitmq:5672/
      - WORKER_QUEUE=ingestion.tasks
      - WORKER_RETRY_QUEUE=ingestion.tasks.retry
      - WORKER_DLQ_QUEUE=ingestion.tasks.dlq
      - WORKER_MAX_ATTEMPTS=5
      - WORKER_PREFETCH=5
      - WORKER_METRICS_PORT=8083
      - WORKER_STORAGE_URL=http://document-storage:8081
      - WORKER_RETRIEVAL_URL=http://retrieval:8080
      - WORKER_DOC_PROCESSOR_URL=http://doc-processor:8082
      - WORKER_LOG_LEVEL=INFO
    depends_on:
      rabbitmq:
        condition: service_healthy
      document-storage:
        condition: service_started
      retrieval:
        condition: service_started
      doc-processor:
        condition: service_started
    networks:
      - rag-network

  # RAG LLM gate (calls retrieval + LLM)
  rag-gate:
    build:
      context: ./gate
    ports:
      - "8090:8090"
    environment:
      # Use host-published retrieval port to avoid docker-network response issues
      - GATE_RETRIEVAL_URL=http://host.docker.internal:8080
      - GATE_RETRIEVAL_TIMEOUT_S=60
      - GATE_RETRIEVAL_MODE=hybrid
      - GATE_TOP_K=8
      - GATE_MAX_CONTEXT_CHARS=18000
      # Multi-hop retrieval improvements (multi-query + 2-pass). Enable for ITMO.
      - GATE_MULTI_QUERY_ENABLED=true
      - GATE_MULTI_QUERY_MAX_QUERIES=4
      - GATE_MULTI_QUERY_TOP_K_MULTIPLIER=12
      - GATE_MULTI_QUERY_RRF_K=60
      - GATE_TWO_PASS_ENABLED=true
      - GATE_TWO_PASS_HINT_MAX_TERMS=8
      - GATE_TWO_PASS_MIN_UNIQUE_DOCS=3
      # Document Storage
      - GATE_STORAGE_URL=http://document-storage:8081
      - GATE_STORAGE_TIMEOUT_S=60
      # Document Processor
      - GATE_DOC_PROCESSOR_URL=http://doc-processor:8082
      - GATE_DOC_PROCESSOR_TIMEOUT_S=300
      # Async ingestion queue
      - GATE_RABBIT_URL=amqp://${RABBITMQ_DEFAULT_USER:-guest}:${RABBITMQ_DEFAULT_PASS:-guest}@rabbitmq:5672/
      - GATE_RABBIT_QUEUE=ingestion.tasks
      # LLM (OpenAI-compatible). Keep mock as default to avoid requiring a key by default.
      - GATE_LLM_PROVIDER=${GATE_LLM_PROVIDER:-openai_compat}
      - GATE_LLM_BASE_URL=${GATE_LLM_BASE_URL:-https://inference.asicloud.cudos.org/v1}
      - GATE_LLM_MODEL=${GATE_LLM_MODEL:-meta-llama/llama-3.3-70b-instruct}
      - GATE_LLM_API_KEY=${GATE_LLM_API_KEY:-}
      - GATE_LOG_LEVEL=INFO
    depends_on:
      retrieval:
        condition: service_started
      document-storage:
        condition: service_started
      rabbitmq:
        condition: service_healthy
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - rag-network

  # Simple frontend (nginx static + proxy /api to rag-gate)
  ui:
    build:
      context: ./ui
    ports:
      - "3300:80"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      rag-gate:
        condition: service_started
    networks:
      - rag-network

  # Rerank service (Infinity). Kept in the same docker network for stable DNS (infinity-rerank).
  infinity-rerank:
    image: michaelf34/infinity:latest
    container_name: infinity-rerank
    command:
      [
        "v2",
        "--model-id",
        "BAAI/bge-reranker-v2-m3",
        "--device",
        "cuda",
        "--device-id",
        "0",
        "--port",
        "7998"
      ]
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HF_HOME=/root/.cache/huggingface
      - DO_NOT_TRACK=1
    networks:
      - rag-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["4"]
              capabilities: [gpu]

  prometheus:
    image: prom/prometheus:v2.55.0
    container_name: prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=15d"
    ports:
      - "9090:9090"
    volumes:
      - prometheus-data:/prometheus
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    networks:
      - rag-network

  grafana:
    image: grafana/grafana:11.3.1
    container_name: grafana
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    ports:
      - "3000:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    networks:
      - rag-network

volumes:
  opensearch-data:
    driver: local
  qdrant-data:
    driver: local
  postgres-data:
    driver: local
  document-storage-data:
    driver: local
  rustfs_data_0:
    driver: local
  rustfs_data_1:
    driver: local
  rustfs_data_2:
    driver: local
  rustfs_data_3:
    driver: local
  rustfs_logs:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local

networks:
  rag-network:
    driver: bridge
  rustfs-network:
    driver: bridge

