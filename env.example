# Copy to ".env" locally (do NOT commit secrets)
# Enable real LLM:
#   GATE_LLM_PROVIDER=openai_compat
#   GATE_LLM_API_KEY=...
GATE_LLM_PROVIDER=mock
GATE_LLM_BASE_URL=https://inference.asicloud.cudos.org/v1
GATE_LLM_MODEL=meta-llama/llama-3.3-70b-instruct
GATE_LLM_API_KEY=

# ---- Retrieval service (service/) ----
# Embeddings backend (required when RAG_EMBEDDING_PROVIDER=http)
# RAG_EMBEDDING_URL=http://host.docker.internal:7997/embeddings
# RAG_EMBEDDING_MODEL=
# RAG_EMBEDDING_API_KEY=

# Contextual Chunk Headers (CCH) for embeddings (opt-in).
# Prepends lightweight metadata headers to text BEFORE embedding (vector search),
# while keeping stored chunk text unchanged.
#
# IMPORTANT: enabling this changes embedding inputs => requires re-index/re-embed.
RAG_EMBEDDING_CONTEXTUAL_HEADERS_ENABLED=false
RAG_EMBEDDING_CONTEXTUAL_HEADERS_MAX_CHARS=400








